/*
THIS IS A GENERATED/BUNDLED FILE BY ESBUILD
if you want to view the source, please visit the github repository of this plugin
*/

var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/main.ts
var main_exports = {};
__export(main_exports, {
  default: () => NeuroVoxPlugin
});
module.exports = __toCommonJS(main_exports);
var import_obsidian8 = require("obsidian");

// src/settings/Settings.ts
var import_obsidian = require("obsidian");
var DEFAULT_SETTINGS = {
  openaiApiKey: "",
  openaiModel: "gpt-4o",
  maxTokens: 500,
  generateAudioSummary: false,
  voiceChoice: "onyx",
  prompt: "Summarize the following transcript concisely, capturing the main points and key details.",
  voiceSpeed: 1,
  saveRecording: true,
  enableVoiceGeneration: false,
  recordingFolderPath: "Recordings",
  voiceMode: "standard"
};

// src/settings/SettingTab.ts
var import_obsidian2 = require("obsidian");
var NeuroVoxSettingTab = class extends import_obsidian2.PluginSettingTab {
  /**
   * Constructs a new instance of the class.
   * @param app The main application object.
   * @param plugin The NeuroVoxPlugin instance to be used with this class.
   */
  constructor(app, plugin) {
    super(app, plugin);
    this.plugin = plugin;
  }
  /**
   * Displays the settings UI for the plugin.
   * This method initializes and renders various settings components allowing the user to configure the plugin.
   * Each setting component is responsible for a specific configuration option, such as API keys, model selection,
   * token limits, prompt settings, and audio settings.
   */
  display() {
    const { containerEl } = this;
    containerEl.empty();
    new import_obsidian2.Setting(containerEl).setName("OpenAI API Key").setDesc("Enter your OpenAI API Key").addText((text) => {
      text.inputEl.type = "password";
      text.setPlaceholder("Enter API Key").setValue(this.plugin.settings.openaiApiKey).onChange(async (value) => {
        this.plugin.settings.openaiApiKey = value;
        await this.plugin.saveSettings();
      });
    });
    new import_obsidian2.Setting(containerEl).setName("OpenAI Model").setDesc("Select the OpenAI Model to use").addText(
      (text) => text.setPlaceholder("Enter model name").setValue(this.plugin.settings.openaiModel).onChange(async (value) => {
        this.plugin.settings.openaiModel = value;
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian2.Setting(containerEl).setName(`Max Tokens (${this.plugin.settings.maxTokens})`).setDesc("Maximum number of tokens").addSlider((slider) => {
      slider.setLimits(0, 128e3, 500).setValue(this.plugin.settings.maxTokens).onChange(async (value) => {
        this.plugin.settings.maxTokens = value;
        this.display();
        await this.plugin.saveSettings();
      });
    });
    new import_obsidian2.Setting(containerEl).setName("Prompt").setDesc("This prompt is used when generating summaries.").addTextArea((text) => {
      text.inputEl.style.width = "100%";
      text.inputEl.style.height = "64px";
      text.inputEl.style.resize = "vertical";
      text.setPlaceholder("Enter prompt").setValue(this.plugin.settings.prompt).onChange(async (value) => {
        this.plugin.settings.prompt = value;
        await this.plugin.saveSettings();
      });
    });
    new import_obsidian2.Setting(containerEl).setName("Generate Audio Summary").setDesc("Enable or disable audio summary generation").addToggle(
      (toggle) => toggle.setValue(this.plugin.settings.generateAudioSummary).onChange(async (value) => {
        this.plugin.settings.generateAudioSummary = value;
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian2.Setting(containerEl).setName("Enable AI Voice Generation of Summaries").setDesc(
      "Whether to enable AI voice generation from transcription summaries"
    ).addToggle(
      (toggle) => toggle.setValue(this.plugin.settings.enableVoiceGeneration).onChange(async (value) => {
        this.plugin.settings.enableVoiceGeneration = value;
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian2.Setting(containerEl).setName("Voice Choice").setDesc("Select the voice for audio summary").addDropdown((dropdown) => {
      dropdown.addOption("alloy", "Alloy").addOption("echo", "Echo").addOption("fable", "Fable").addOption("onyx", "Onyx").addOption("nova", "Nova").addOption("shimmer", "Shimmer").setValue(this.plugin.settings.voiceChoice).onChange(async (value) => {
        this.plugin.settings.voiceChoice = value;
        await this.plugin.saveSettings();
      });
    });
    new import_obsidian2.Setting(containerEl).setName(`Voice Speed (${this.plugin.settings.voiceSpeed})`).setDesc("Set the speed of the voice").addSlider((slider) => {
      slider.setLimits(0.25, 4, 0.25).setValue(this.plugin.settings.voiceSpeed).onChange(async (value) => {
        this.plugin.settings.voiceSpeed = value;
        this.display();
        await this.plugin.saveSettings();
      });
    });
    new import_obsidian2.Setting(containerEl).setName("Enable HD Voice").setDesc("When enabled, use HD voice for audio summaries").addToggle(
      (toggle) => toggle.setValue(this.plugin.settings.voiceMode === "hd").onChange(async (value) => {
        this.plugin.settings.voiceMode = value ? "hd" : "standard";
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian2.Setting(containerEl).setName("Save Recording").setDesc("Enable or disable saving recordings").addToggle(
      (toggle) => toggle.setValue(this.plugin.settings.saveRecording).onChange(async (value) => {
        this.plugin.settings.saveRecording = value;
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian2.Setting(containerEl).setName("Recording Folder Path").setDesc(
      'Specify the folder path to save recordings relative to the vault root. For a folder in the root directory, enter its name (e.g., "Recordings"). For a nested folder, use forward slashes to indicate the path (e.g., "Audio/Recordings").'
    ).addText(
      (text) => text.setPlaceholder("Enter folder path").setValue(this.plugin.settings.recordingFolderPath).onChange(async (value) => {
        this.plugin.settings.recordingFolderPath = value;
        await this.plugin.saveSettings();
      })
    );
  }
};

// src/processors/RecordBlockProcessor.ts
var import_obsidian7 = require("obsidian");

// src/ui/FloatingButton.ts
var import_obsidian6 = require("obsidian");

// src/modals/TimerModal.ts
var import_obsidian3 = require("obsidian");

// src/utils/SvgUtils.ts
function createButtonWithSvgIcon(svgText) {
  const parser = new DOMParser();
  const svgDoc = parser.parseFromString(svgText, "image/svg+xml");
  const svgElement = svgDoc.documentElement;
  const button = document.createElement("button");
  button.appendChild(svgElement);
  return button;
}

// src/assets/icons.ts
var icons = {
  microphone: `<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mic"><path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z"/><path d="M19 10v2a7 7 0 0 1-14 0v-2"/><line x1="12" x2="12" y1="19" y2="22"/></svg>`,
  pause: `<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-pause"><rect x="14" y="4" width="4" height="16" rx="1"/><rect x="6" y="4" width="4" height="16" rx="1"/></svg>`,
  play: `<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-play"><polygon points="6 3 20 12 6 21 6 3"/></svg>`,
  stop: `<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-square"><rect width="18" height="18" x="3" y="3" rx="2"/></svg>`
};

// src/modals/TimerModal.ts
var TimerModal = class extends import_obsidian3.Modal {
  constructor(app) {
    super(app);
    this.intervalId = null;
    this.seconds = 0;
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.recordingStopped = false;
    this.isRecording = false;
    this.isPaused = false;
  }
  onOpen() {
    const { contentEl } = this;
    contentEl.empty();
    contentEl.addClass("neurovox-modal");
    const modalContent = contentEl.createDiv({ cls: "neurovox-modal-content" });
    this.timerEl = modalContent.createEl("div", { cls: "neurovox-timer", text: "00:00" });
    const buttonGroup = modalContent.createEl("div", { cls: "neurovox-button-group" });
    this.recordButton = createButtonWithSvgIcon(icons.microphone);
    this.pauseButton = createButtonWithSvgIcon(icons.pause);
    this.stopButton = createButtonWithSvgIcon(icons.stop);
    this.recordButton.addClass("neurovox-button", "neurovox-record-button");
    this.pauseButton.addClass("neurovox-button", "neurovox-pause-button");
    this.stopButton.addClass("neurovox-button", "neurovox-stop-button");
    this.pauseButton.style.display = "none";
    buttonGroup.appendChild(this.recordButton);
    buttonGroup.appendChild(this.pauseButton);
    buttonGroup.appendChild(this.stopButton);
    this.recordButton.addEventListener("click", () => this.toggleRecording());
    this.pauseButton.addEventListener("click", () => this.togglePause());
    this.stopButton.addEventListener("click", () => this.stopRecording());
    this.startRecording();
  }
  onClose() {
    if (!this.recordingStopped) {
      this.stopRecording();
    }
  }
  async toggleRecording() {
    if (this.isRecording) {
      this.pauseRecording();
    } else {
      this.startRecording();
    }
  }
  async startRecording() {
    this.isRecording = true;
    this.isPaused = false;
    this.recordButton.addClass("recording");
    this.pauseButton.style.display = "flex";
    this.recordButton.innerHTML = icons.microphone;
    if (!this.intervalId) {
      this.intervalId = window.setInterval(() => {
        this.seconds++;
        this.updateTimerDisplay();
      }, 1e3);
    }
    if (!this.mediaRecorder) {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      this.mediaRecorder = new MediaRecorder(stream);
      this.mediaRecorder.ondataavailable = (event) => {
        this.audioChunks.push(event.data);
      };
      this.mediaRecorder.start();
    } else {
      this.mediaRecorder.resume();
    }
  }
  togglePause() {
    if (this.isPaused) {
      this.resumeRecording();
    } else {
      this.pauseRecording();
    }
  }
  pauseRecording() {
    if (this.mediaRecorder) {
      this.mediaRecorder.pause();
    }
    if (this.intervalId) {
      window.clearInterval(this.intervalId);
      this.intervalId = null;
    }
    this.isRecording = false;
    this.isPaused = true;
    this.recordButton.removeClass("recording");
    this.pauseButton.innerHTML = icons.play;
  }
  resumeRecording() {
    this.startRecording();
    this.pauseButton.innerHTML = icons.pause;
  }
  stopRecording() {
    if (this.recordingStopped)
      return;
    this.recordingStopped = true;
    if (this.mediaRecorder) {
      this.mediaRecorder.onstop = () => {
        const audioBlob = new Blob(this.audioChunks, { type: "audio/mp3" });
        this.audioChunks = [];
        if (this.onStop) {
          this.onStop(audioBlob);
        }
        this.close();
      };
      this.mediaRecorder.stop();
      this.mediaRecorder.stream.getTracks().forEach((track) => track.stop());
      this.mediaRecorder = null;
    }
    if (this.intervalId) {
      window.clearInterval(this.intervalId);
      this.intervalId = null;
    }
    this.seconds = 0;
    this.updateTimerDisplay();
    this.recordButton.removeClass("recording");
    this.pauseButton.style.display = "none";
  }
  updateTimerDisplay() {
    const minutes = Math.floor(this.seconds / 60).toString().padStart(2, "0");
    const seconds = (this.seconds % 60).toString().padStart(2, "0");
    this.timerEl.textContent = `${minutes}:${seconds}`;
  }
};

// src/processors/openai.ts
var import_obsidian4 = require("obsidian");
var API_BASE_URL = "https://api.openai.com/v1";
var WHISPER_MODEL = "whisper-1";
var TTS_MODEL = "tts-1";
async function sendOpenAIRequest(endpoint, body, settings, isJson = true) {
  console.log(`Sending request to ${endpoint}`);
  console.log(`API Key: ${settings.openaiApiKey.substring(0, 5)}...`);
  const headers = {
    "Authorization": `Bearer ${settings.openaiApiKey}`
  };
  if (isJson) {
    headers["Content-Type"] = "application/json";
    body = JSON.stringify(body);
  }
  try {
    const response = await (0, import_obsidian4.requestUrl)({
      url: endpoint,
      method: "POST",
      headers,
      body
    });
    console.log(`Response status: ${response.status}`);
    if (response.status !== 200) {
      console.error("Response text:", response.text);
      throw new Error(`OpenAI API request failed: ${response.status}`);
    }
    return JSON.parse(response.text);
  } catch (error) {
    console.error("Error in sendOpenAIRequest:", error);
    throw error;
  }
}
async function transcribeAudio(audioBlob, settings) {
  const endpoint = `${API_BASE_URL}/audio/transcriptions`;
  const formData = new FormData();
  formData.append("file", audioBlob, "audio.mp3");
  formData.append("model", WHISPER_MODEL);
  try {
    const result = await sendOpenAIRequest(endpoint, formData, settings, false);
    return result.text;
  } catch (error) {
    console.error("Transcription error:", error);
    throw new Error(`Failed to transcribe audio: ${error.message}`);
  }
}
async function generateChatCompletion(transcript, settings) {
  const endpoint = `${API_BASE_URL}/chat/completions`;
  try {
    const result = await sendOpenAIRequest(endpoint, {
      model: settings.openaiModel,
      messages: [
        { role: "system", content: settings.prompt },
        { role: "user", content: transcript }
      ],
      max_tokens: settings.maxTokens
    }, settings);
    return result.choices[0].message.content;
  } catch (error) {
    console.error("Chat completion error:", error);
    throw new Error(`Failed to generate chat completion: ${error.message}`);
  }
}
async function generateSpeech(text, settings) {
  const endpoint = `${API_BASE_URL}/audio/speech`;
  try {
    const result = await sendOpenAIRequest(endpoint, {
      model: TTS_MODEL,
      input: text,
      voice: settings.voiceChoice,
      response_format: "mp3",
      speed: settings.voiceSpeed
    }, settings);
    return new Blob([result], { type: "audio/mp3" });
  } catch (error) {
    console.error("Speech generation error:", error);
    throw new Error(`Failed to generate speech: ${error.message}`);
  }
}

// src/utils/FileUtils.ts
var import_obsidian5 = require("obsidian");
async function saveAudioFile(app, audioBlob, fileName, settings) {
  const fileManager = app.fileManager;
  let filePath = await fileManager.getAvailablePathForAttachment(fileName);
  const folderPath = settings.recordingFolderPath;
  filePath = `${folderPath}/${filePath}`;
  const folder = app.vault.getAbstractFileByPath(folderPath);
  if (!folder) {
    console.log(`Folder ${folderPath} does not exist. Creating...`);
    await app.vault.createFolder(folderPath);
  } else if (!(folder instanceof import_obsidian5.TFolder)) {
    throw new Error(`${folderPath} is not a folder`);
  }
  const arrayBuffer = await audioBlob.arrayBuffer();
  const uint8Array = new Uint8Array(arrayBuffer);
  const existingFile = app.vault.getAbstractFileByPath(filePath);
  if (existingFile) {
    await app.vault.delete(existingFile);
  }
  const file = await app.vault.createBinary(filePath, uint8Array);
  console.log(`Saved recording as ${file.path}`);
  return file;
}

// src/ui/FloatingButton.ts
var FloatingButton = class {
  constructor(plugin, settings, contentContainer) {
    this.plugin = plugin;
    this.settings = settings;
    this.contentContainer = contentContainer;
    this.createButton();
  }
  createButton() {
    this.buttonEl = createButtonWithSvgIcon(icons.microphone);
    this.buttonEl.addClass("neurovox-button", "floating");
    this.buttonEl.addEventListener("click", () => this.openRecordingModal());
  }
  openRecordingModal() {
    const modal = new TimerModal(this.plugin.app);
    modal.onStop = async (audioBlob) => {
      await this.processRecording(audioBlob);
    };
    modal.open();
  }
  async processRecording(audioBlob) {
    try {
      console.log("Processing recording started");
      console.log(`Audio blob size: ${audioBlob.size} bytes`);
      console.log(`Audio blob type: ${audioBlob.type}`);
      const fileName = `recording-${Date.now()}.mp3`;
      const filePath = `${this.settings.recordingFolderPath}/${fileName}`;
      const file = await saveAudioFile(this.plugin.app, audioBlob, filePath, this.settings);
      console.log(`Saved recording as ${file.path}`);
      console.log("Starting transcription");
      const transcription = await transcribeAudio(audioBlob, this.settings);
      console.log("Transcription completed:", transcription);
      console.log("Generating summary");
      const summary = await generateChatCompletion(transcription, this.settings);
      console.log("Summary generated:", summary);
      let audioSummaryFile = null;
      if (this.settings.enableVoiceGeneration) {
        console.log("Generating audio summary");
        const audioSummaryBlob = await generateSpeech(summary, this.settings);
        const summaryFileName = `summary-${Date.now()}.mp3`;
        const summaryFilePath = `${this.settings.recordingFolderPath}/${summaryFileName}`;
        audioSummaryFile = await saveAudioFile(this.plugin.app, audioSummaryBlob, summaryFilePath, this.settings);
        console.log("Audio summary generated:", audioSummaryFile.path);
      }
      this.updateRecordBlockContent(file, transcription, summary, audioSummaryFile);
      this.removeButton();
      new import_obsidian6.Notice("Recording processed successfully");
    } catch (error) {
      console.error("Error processing recording:", error);
      new import_obsidian6.Notice("Error processing recording. Check console for details.");
    }
  }
  updateRecordBlockContent(audioFile, transcription, summary, audioSummaryFile) {
    const content = `
## Generations
${audioSummaryFile ? `![[${audioSummaryFile.path}]]
` : ""}
${summary}

## Transcript
![[${audioFile.path}]]
${transcription}
        `;
    this.contentContainer.innerHTML = content;
  }
  removeButton() {
    if (this.buttonEl && this.buttonEl.parentNode) {
      this.buttonEl.parentNode.removeChild(this.buttonEl);
    }
  }
};

// src/processors/RecordBlockProcessor.ts
function registerRecordBlockProcessor(plugin, settings) {
  plugin.registerMarkdownCodeBlockProcessor("record", (source, el, ctx) => {
    const contentContainer = el.createDiv({ cls: "neurovox-record-content" });
    const floatingButton = new FloatingButton(plugin, settings, contentContainer);
    el.appendChild(floatingButton.buttonEl);
    el.neurovoxContentContainer = contentContainer;
    el.neurovoxFloatingButton = floatingButton;
    ctx.addChild(new class extends import_obsidian7.MarkdownRenderChild {
      constructor(containerEl) {
        super(containerEl);
      }
      onunload() {
        floatingButton.removeButton();
      }
    }(el));
  });
}

// src/main.ts
var NeuroVoxPlugin = class extends import_obsidian8.Plugin {
  /**
   * Runs when the plugin is loaded.
   * Initializes settings, UI components, and sets up event listeners.
   */
  async onload() {
    console.log("Loading NeuroVox plugin");
    await this.loadSettings();
    registerRecordBlockProcessor(this, this.settings);
    this.addSettingTab(new NeuroVoxSettingTab(this.app, this));
    this.registerView(
      "neurovox-view",
      (leaf) => new NeuroVoxView(leaf)
    );
    this.addRibbonIcon("microphone", "NeuroVox", () => {
      this.activateView();
    });
    this.addCommand({
      id: "open-neurovox-view",
      name: "Open NeuroVox View",
      callback: () => {
        this.activateView();
      }
    });
  }
  /**
   * Runs when the plugin is unloaded.
   * Performs cleanup tasks.
   */
  onunload() {
    console.log("Unloading NeuroVox plugin");
  }
  /**
   * Loads the plugin settings.
   * Merges saved settings with default settings.
   */
  async loadSettings() {
    this.settings = Object.assign({}, DEFAULT_SETTINGS, await this.loadData());
  }
  /**
   * Saves the current plugin settings.
   */
  async saveSettings() {
    await this.saveData(this.settings);
  }
  /**
   * Activates the NeuroVox view.
   * Creates a new leaf for the view if it doesn't exist, or reveals an existing one.
   */
  async activateView() {
    const { workspace } = this.app;
    let leaf = workspace.getLeavesOfType("neurovox-view")[0];
    if (!leaf) {
      const newLeaf = workspace.getRightLeaf(false);
      if (newLeaf) {
        await newLeaf.setViewState({ type: "neurovox-view", active: true });
        leaf = newLeaf;
      } else {
        console.error("Failed to create a new leaf for NeuroVox view");
        return;
      }
    }
    workspace.revealLeaf(leaf);
  }
};
var NeuroVoxView = class extends import_obsidian8.ItemView {
  constructor(leaf) {
    super(leaf);
  }
  /**
   * Returns the type identifier for this view.
   */
  getViewType() {
    return "neurovox-view";
  }
  /**
   * Returns the display text for this view.
   */
  getDisplayText() {
    return "NeuroVox";
  }
  /**
   * Renders the content of the NeuroVox view.
   */
  async onOpen() {
    const container = this.containerEl.children[1];
    container.empty();
    container.createEl("h4", { text: "Welcome to NeuroVox" });
  }
  /**
   * Performs any necessary cleanup when the view is closed.
   */
  async onClose() {
  }
};
