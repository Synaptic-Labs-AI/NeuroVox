/*
THIS IS A GENERATED/BUNDLED FILE BY ESBUILD
if you want to view the source, please visit the github repository of this plugin
*/

var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/main.ts
var main_exports = {};
__export(main_exports, {
  default: () => NeuroVoxPlugin
});
module.exports = __toCommonJS(main_exports);
var import_obsidian7 = require("obsidian");

// src/settings/Settings.ts
var import_obsidian = require("obsidian");
var DEFAULT_SETTINGS = {
  openaiApiKey: "",
  openaiModel: "gpt-4o",
  maxTokens: 500,
  generateAudioSummary: false,
  voiceChoice: "onyx",
  prompt: "Summarize the following transcript concisely, capturing the main points and key details.",
  voiceSpeed: 1,
  saveRecording: true,
  enableVoiceGeneration: false,
  recordingFolderPath: "Recordings",
  voiceMode: "standard",
  micButtonColor: "#4B4B4B"
};

// src/settings/SettingTab.ts
var import_obsidian2 = require("obsidian");
var NeuroVoxSettingTab = class extends import_obsidian2.PluginSettingTab {
  constructor(app, plugin) {
    super(app, plugin);
    this.plugin = plugin;
  }
  display() {
    const { containerEl } = this;
    containerEl.empty();
    containerEl.createEl("h2", { text: "NeuroVox Settings" });
    new import_obsidian2.Setting(containerEl).setName("OpenAI API Key").setDesc("Enter your OpenAI API key").addText((text) => text.setPlaceholder("API Key").setValue(this.plugin.settings.openaiApiKey).onChange(async (value) => {
      this.plugin.settings.openaiApiKey = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian2.Setting(containerEl).setName("OpenAI Model").setDesc("Enter the OpenAI model to use for content generation. See the available models here: https://platform.openai.com/docs/models").addText((text) => text.setPlaceholder("Model Name").setValue(this.plugin.settings.openaiModel).onChange(async (value) => {
      this.plugin.settings.openaiModel = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian2.Setting(containerEl).setName("Max Tokens").setDesc("Set the maximum number of tokens for chat completions").addText((text) => text.setPlaceholder("Max Tokens").setValue(this.plugin.settings.maxTokens.toString()).onChange(async (value) => {
      this.plugin.settings.maxTokens = parseInt(value);
      await this.plugin.saveSettings();
    }));
    new import_obsidian2.Setting(containerEl).setName("Prompt").setDesc("Enter the prompt to use for content generation").addTextArea((text) => text.setPlaceholder("Prompt").setValue(this.plugin.settings.prompt).onChange(async (value) => {
      this.plugin.settings.prompt = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian2.Setting(containerEl).setName("Voice Speed").setDesc("Set the speed of the generated speech (0.25 to 4.0)").addText((text) => text.setPlaceholder("Voice Speed").setValue(this.plugin.settings.voiceSpeed.toString()).onChange(async (value) => {
      this.plugin.settings.voiceSpeed = parseFloat(value);
      await this.plugin.saveSettings();
    }));
    new import_obsidian2.Setting(containerEl).setName("Enable AI Voice Generation of Summaries").setDesc("Whether to enable AI voice generation from transcription summaries").addToggle((toggle) => toggle.setValue(this.plugin.settings.enableVoiceGeneration).onChange(async (value) => {
      this.plugin.settings.enableVoiceGeneration = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian2.Setting(containerEl).setName("Enable HD Voice").setDesc("When enabled, use HD voice for audio summaries").addToggle((toggle) => toggle.setValue(this.plugin.settings.voiceMode === "hd").onChange(async (value) => {
      this.plugin.settings.voiceMode = value ? "hd" : "standard";
      await this.plugin.saveSettings();
    }));
    new import_obsidian2.Setting(containerEl).setName("Save Recording").setDesc("Enable or disable saving recordings").addToggle((toggle) => toggle.setValue(this.plugin.settings.saveRecording).onChange(async (value) => {
      this.plugin.settings.saveRecording = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian2.Setting(containerEl).setName("Recording Folder Path").setDesc('Specify the folder path to save recordings relative to the vault root. For a folder in the root directory, enter its name (e.g., "Recordings"). For a nested folder, use forward slashes to indicate the path (e.g., "Audio/Recordings").').addText((text) => text.setPlaceholder("Enter folder path").setValue(this.plugin.settings.recordingFolderPath).onChange(async (value) => {
      this.plugin.settings.recordingFolderPath = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian2.Setting(containerEl).setName("Microphone Button Color").setDesc("Choose a color for the microphone button").addColorPicker((colorPicker) => colorPicker.setValue(this.plugin.settings.micButtonColor).onChange(async (value) => {
      this.plugin.settings.micButtonColor = value;
      await this.plugin.saveSettings();
      this.updateMicButtonColor(value);
    }));
  }
  updateMicButtonColor(color) {
    document.documentElement.style.setProperty("--mic-button-color", color);
  }
};

// src/processors/RecordBlockProcessor.ts
var import_obsidian6 = require("obsidian");

// src/ui/FloatingButton.ts
var import_obsidian5 = require("obsidian");

// src/modals/TimerModal.ts
var import_obsidian3 = require("obsidian");

// src/utils/SvgUtils.ts
function createButtonWithSvgIcon(svgText) {
  const parser = new DOMParser();
  const svgDoc = parser.parseFromString(svgText, "image/svg+xml");
  const svgElement = svgDoc.documentElement;
  const button = document.createElement("button");
  button.appendChild(svgElement);
  return button;
}

// src/assets/icons.ts
var icons = {
  microphone: `<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mic"><path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z"/><path d="M19 10v2a7 7 0 0 1-14 0v-2"/><line x1="12" x2="12" y1="19" y2="22"/></svg>`,
  pause: `<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-pause"><rect x="14" y="4" width="4" height="16" rx="1"/><rect x="6" y="4" width="4" height="16" rx="1"/></svg>`,
  play: `<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-play"><polygon points="6 3 20 12 6 21 6 3"/></svg>`,
  stop: `<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-square"><rect width="18" height="18" x="3" y="3" rx="2"/></svg>`
};

// src/modals/TimerModal.ts
var TimerModal = class extends import_obsidian3.Modal {
  constructor(app) {
    super(app);
    this.intervalId = null;
    this.seconds = 0;
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.recordingStopped = false;
    this.isRecording = false;
    this.isPaused = false;
  }
  onOpen() {
    const { contentEl } = this;
    contentEl.empty();
    contentEl.addClass("neurovox-modal");
    const modalContent = contentEl.createDiv({ cls: "neurovox-modal-content" });
    const timerGroup = modalContent.createEl("div", { cls: "neurovox-timer-group" });
    this.timerEl = timerGroup.createEl("div", { cls: "neurovox-timer", text: "00:00" });
    this.pulsingButton = document.createElement("button");
    this.pulsingButton.addClass("neurovox-button", "pulsing");
    timerGroup.appendChild(this.pulsingButton);
    const buttonGroup = modalContent.createEl("div", { cls: "neurovox-button-group" });
    this.pauseButton = createButtonWithSvgIcon(icons.pause);
    this.stopButton = createButtonWithSvgIcon(icons.stop);
    this.pauseButton.addClass("neurovox-button", "neurovox-pause-button");
    this.stopButton.addClass("neurovox-button", "neurovox-stop-button");
    buttonGroup.appendChild(this.pauseButton);
    buttonGroup.appendChild(this.stopButton);
    this.pauseButton.addEventListener("click", () => this.togglePause());
    this.stopButton.addEventListener("click", () => this.stopRecording());
    this.startRecording();
  }
  onClose() {
    if (!this.recordingStopped) {
      this.stopRecording();
    }
  }
  async togglePause() {
    if (this.isPaused) {
      this.resumeRecording();
    } else {
      this.pauseRecording();
    }
  }
  async startRecording() {
    this.isRecording = true;
    this.isPaused = false;
    this.pulsingButton.style.display = "block";
    this.pauseButton.style.display = "block";
    if (!this.intervalId) {
      this.intervalId = window.setInterval(() => {
        this.seconds++;
        this.updateTimerDisplay();
      }, 1e3);
    }
    if (!this.mediaRecorder) {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      this.mediaRecorder = new MediaRecorder(stream);
      this.mediaRecorder.ondataavailable = (event) => {
        this.audioChunks.push(event.data);
      };
      this.mediaRecorder.start();
    } else {
      this.mediaRecorder.resume();
    }
  }
  pauseRecording() {
    if (this.mediaRecorder) {
      this.mediaRecorder.pause();
    }
    if (this.intervalId) {
      window.clearInterval(this.intervalId);
      this.intervalId = null;
    }
    this.isRecording = false;
    this.isPaused = true;
    this.pulsingButton.style.display = "none";
    this.pauseButton.innerHTML = icons.play;
  }
  resumeRecording() {
    this.startRecording();
    this.pauseButton.innerHTML = icons.pause;
  }
  stopRecording() {
    if (this.recordingStopped)
      return;
    this.recordingStopped = true;
    if (this.mediaRecorder) {
      this.mediaRecorder.onstop = () => {
        const audioBlob = new Blob(this.audioChunks, { type: "audio/wav" });
        this.audioChunks = [];
        if (this.onStop) {
          this.onStop(audioBlob);
        }
        this.close();
      };
      this.mediaRecorder.stop();
      this.mediaRecorder.stream.getTracks().forEach((track) => track.stop());
      this.mediaRecorder = null;
    }
    if (this.intervalId) {
      window.clearInterval(this.intervalId);
      this.intervalId = null;
    }
    this.seconds = 0;
    this.updateTimerDisplay();
    this.pulsingButton.style.display = "none";
    this.pauseButton.style.display = "none";
  }
  updateTimerDisplay() {
    const minutes = Math.floor(this.seconds / 60).toString().padStart(2, "0");
    const seconds = (this.seconds % 60).toString().padStart(2, "0");
    this.timerEl.textContent = `${minutes}:${seconds}`;
  }
};

// src/processors/openai.ts
var API_BASE_URL = "https://api.openai.com/v1";
var WHISPER_MODEL = "whisper-1";
var TTS_MODEL = "tts-1";
async function sendOpenAIRequest(endpoint, body, settings, isFormData = false, isBinaryResponse = false) {
  console.log(`[sendOpenAIRequest] Starting request to ${endpoint}`);
  let requestBody;
  let headers = {
    "Authorization": `Bearer ${settings.openaiApiKey}`
  };
  if (isFormData) {
    requestBody = new FormData();
    requestBody.append("file", body, "audio.wav");
    requestBody.append("model", WHISPER_MODEL);
  } else {
    requestBody = JSON.stringify(body);
    headers["Content-Type"] = "application/json";
  }
  const url = `${API_BASE_URL}${endpoint}`;
  try {
    let response;
    if (isFormData) {
      response = await fetch(url, {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${settings.openaiApiKey}`
        },
        body: requestBody
      });
    } else {
      response = await fetch(url, {
        method: "POST",
        headers,
        body: requestBody
      });
    }
    console.log(`[sendOpenAIRequest] Response status: ${response.status}`);
    if (!response.ok) {
      const errorText = await response.text();
      console.error("[sendOpenAIRequest] Error response:", errorText);
      throw new Error(`OpenAI API request failed: ${response.status} - ${errorText}`);
    }
    if (isBinaryResponse) {
      return await response.arrayBuffer();
    } else {
      return await response.json();
    }
  } catch (error) {
    console.error("[sendOpenAIRequest] Error:", error);
    throw error;
  }
}
async function transcribeAudio(audioBlob, settings) {
  console.log("[transcribeAudio] Starting transcription");
  if (!(audioBlob instanceof Blob)) {
    throw new Error("Invalid input: audioBlob must be a Blob object");
  }
  if (audioBlob.size === 0) {
    throw new Error("Invalid input: audioBlob is empty");
  }
  console.log(`[transcribeAudio] Audio blob size: ${audioBlob.size} bytes`);
  console.log(`[transcribeAudio] Audio blob type: ${audioBlob.type}`);
  const endpoint = "/audio/transcriptions";
  try {
    const result = await sendOpenAIRequest(endpoint, audioBlob, settings, true);
    console.log("[transcribeAudio] Transcription result:", result);
    return result.text;
  } catch (error) {
    console.error("[transcribeAudio] Transcription error:", error);
    throw new Error(`Failed to transcribe audio: ${error.message}`);
  }
}
async function generateChatCompletion(transcript, settings) {
  console.log("[generateChatCompletion] Starting chat completion");
  const endpoint = "/chat/completions";
  const maxTokens = Math.min(settings.maxTokens, 4096);
  const requestBody = {
    model: settings.openaiModel,
    messages: [
      { role: "system", content: settings.prompt },
      { role: "user", content: transcript }
    ],
    max_tokens: maxTokens
  };
  try {
    const result = await sendOpenAIRequest(endpoint, requestBody, settings);
    console.log("[generateChatCompletion] Chat completion result:", result);
    return result.choices[0].message.content;
  } catch (error) {
    console.error("[generateChatCompletion] Chat completion error:", error);
    throw new Error(`Failed to generate chat completion: ${error.message}`);
  }
}
async function generateSpeech(text, settings) {
  console.log("[generateSpeech] Starting speech generation");
  const endpoint = "/audio/speech";
  const requestBody = {
    model: TTS_MODEL,
    input: text,
    voice: settings.voiceChoice,
    response_format: "wav",
    speed: settings.voiceSpeed
  };
  try {
    const result = await sendOpenAIRequest(endpoint, requestBody, settings, false, true);
    console.log("[generateSpeech] Speech generation result:", result);
    return result;
  } catch (error) {
    console.error("[generateSpeech] Speech generation error:", error);
    throw new Error(`Failed to generate speech: ${error.message}`);
  }
}

// src/utils/FileUtils.ts
var import_obsidian4 = require("obsidian");
async function saveAudioFile(app, audioBlob, fileName, settings) {
  try {
    const folderPath = settings.recordingFolderPath;
    const filePath = `${folderPath}/${fileName}`;
    console.log(`Attempting to save audio file to path: ${filePath}`);
    await ensureDirectoryExists(app, folderPath);
    const arrayBuffer = await audioBlob.arrayBuffer();
    const uint8Array = new Uint8Array(arrayBuffer);
    console.log(`Writing file to vault at path: ${filePath}`);
    const file = await app.vault.createBinary(filePath, uint8Array);
    if (!file) {
      throw new Error("File creation failed and returned null");
    }
    console.log(`Successfully saved recording as ${file.path}`);
    return file;
  } catch (error) {
    console.error("Error saving audio file:", error);
    throw error;
  }
}
async function ensureDirectoryExists(app, folderPath) {
  const parts = folderPath.split("/");
  let currentPath = "";
  for (const part of parts) {
    currentPath = currentPath ? `${currentPath}/${part}` : part;
    try {
      const folder = app.vault.getAbstractFileByPath(currentPath);
      if (!folder) {
        console.log(`Creating folder: ${currentPath}`);
        await app.vault.createFolder(currentPath);
      } else if (folder instanceof import_obsidian4.TFolder) {
        console.log(`Folder already exists: ${currentPath}`);
      } else {
        throw new Error(`${currentPath} is not a folder`);
      }
    } catch (error) {
      console.error(`Error ensuring directory exists: ${error.message}`);
      throw error;
    }
  }
}

// src/ui/FloatingButton.ts
var FloatingButton = class {
  constructor(plugin, settings) {
    this.plugin = plugin;
    this.settings = settings;
    this.createButton();
    this.contentContainer = document.createElement("div");
    this.registerEventListeners();
  }
  createButton() {
    this.buttonEl = createButtonWithSvgIcon(icons.microphone);
    this.buttonEl.addClass("neurovox-button", "floating");
    this.buttonEl.addEventListener("click", () => this.openRecordingModal());
  }
  appendButtonToCurrentNote() {
    const activeLeaf = this.plugin.app.workspace.activeLeaf;
    if (activeLeaf) {
      const view = activeLeaf.view;
      if (view instanceof import_obsidian5.MarkdownView) {
        const container = view.containerEl;
        const editor = view.editor;
        const doc = editor.getDoc();
        const lines = doc.lineCount();
        let recordBlockFound = false;
        for (let i = 0; i < lines; i++) {
          const line = doc.getLine(i);
          if (line.trim() === "```record") {
            recordBlockFound = true;
            break;
          }
        }
        if (recordBlockFound) {
          container.appendChild(this.buttonEl);
        } else {
          this.removeButton();
        }
      }
    }
  }
  registerEventListeners() {
    this.plugin.app.workspace.on("layout-change", () => {
      this.checkForRecordBlock();
    });
    this.plugin.app.workspace.on("active-leaf-change", () => {
      this.checkForRecordBlock();
    });
    this.plugin.app.workspace.on("editor-change", () => {
      this.checkForRecordBlock();
    });
  }
  checkForRecordBlock() {
    const activeLeaf = this.plugin.app.workspace.activeLeaf;
    if (activeLeaf) {
      const view = activeLeaf.view;
      if (view instanceof import_obsidian5.MarkdownView) {
        const editor = view.editor;
        const doc = editor.getDoc();
        const lines = doc.lineCount();
        let recordBlockFound = false;
        for (let i = 0; i < lines; i++) {
          const line = doc.getLine(i);
          if (line.trim() === "```record") {
            recordBlockFound = true;
            break;
          }
        }
        if (recordBlockFound) {
          this.appendButtonToCurrentNote();
        } else {
          this.removeButton();
        }
      }
    }
  }
  openRecordingModal() {
    const modal = new TimerModal(this.plugin.app);
    modal.onStop = async (audioBlob) => {
      await this.processRecording(audioBlob);
    };
    modal.open();
  }
  async processRecording(audioBlob) {
    try {
      console.log("Processing recording started");
      console.log(`Audio blob size: ${audioBlob.size} bytes`);
      console.log(`Audio blob type: ${audioBlob.type}`);
      const fileName = `recording-${Date.now()}.wav`;
      const file = await saveAudioFile(this.plugin.app, audioBlob, fileName, this.settings);
      console.log(`Saved recording as ${file.path}`);
      console.log("Starting transcription");
      const transcription = await transcribeAudio(audioBlob, this.settings);
      console.log("Transcription completed:", transcription);
      console.log("Generating summary");
      const summary = await generateChatCompletion(transcription, this.settings);
      console.log("Summary generated:", summary);
      let audioSummaryFile = null;
      if (this.settings.enableVoiceGeneration) {
        console.log("Generating audio summary");
        const audioSummaryArrayBuffer = await generateSpeech(summary, this.settings);
        const audioSummaryBlob = new Blob([audioSummaryArrayBuffer], { type: "audio/wav" });
        audioSummaryFile = await saveAudioFile(this.plugin.app, audioSummaryBlob, `summary-${Date.now()}.wav`, this.settings);
        console.log("Audio summary generated:", audioSummaryFile.path);
      }
      this.updateRecordBlockContent(file, transcription, summary, audioSummaryFile);
      const activeLeaf = this.plugin.app.workspace.activeLeaf;
      if (activeLeaf) {
        const view = activeLeaf.view;
        if (view instanceof import_obsidian5.MarkdownView) {
          const editor = view.editor;
          const doc = editor.getDoc();
          const lines = doc.lineCount();
          let recordBlockStart = -1;
          let recordBlockEnd = -1;
          for (let i = 0; i < lines; i++) {
            const line = doc.getLine(i);
            if (line.trim() === "```record") {
              recordBlockStart = i;
            } else if (line.trim() === "```" && recordBlockStart !== -1) {
              recordBlockEnd = i;
              break;
            }
          }
          if (recordBlockStart !== -1 && recordBlockEnd !== -1) {
            const formattedContent = this.formatContent(file, transcription, summary, audioSummaryFile);
            doc.replaceRange(
              formattedContent,
              { line: recordBlockStart, ch: 0 },
              { line: recordBlockEnd, ch: doc.getLine(recordBlockEnd).length }
            );
            this.removeButton();
          }
        }
      }
      new import_obsidian5.Notice("Recording processed successfully");
    } catch (error) {
      console.error("Error processing recording:", error);
      new import_obsidian5.Notice("Failed to process recording");
    }
  }
  formatContent(audioFile, transcription, summary, audioSummaryFile) {
    let content = "## Generations\n";
    if (audioSummaryFile) {
      content += `![[${audioSummaryFile.path}]]
`;
    }
    content += `${summary}

`;
    content += "## Transcription\n";
    content += `![[${audioFile.path}]]
`;
    content += `${transcription}
`;
    return content;
  }
  updateRecordBlockContent(audioFile, transcription, summary, audioSummaryFile) {
    while (this.contentContainer.firstChild) {
      this.contentContainer.removeChild(this.contentContainer.firstChild);
    }
    const generationsHeader = document.createElement("h2");
    generationsHeader.textContent = "Generations";
    this.contentContainer.appendChild(generationsHeader);
    if (audioSummaryFile) {
      const audioSummaryLink = document.createElement("a");
      audioSummaryLink.href = audioSummaryFile.path;
      audioSummaryLink.textContent = audioSummaryFile.path;
      this.contentContainer.appendChild(audioSummaryLink);
      this.contentContainer.appendChild(document.createElement("br"));
    }
    const summaryParagraph = document.createElement("p");
    summaryParagraph.textContent = summary;
    this.contentContainer.appendChild(summaryParagraph);
    const transcriptHeader = document.createElement("h2");
    transcriptHeader.textContent = "Transcript";
    this.contentContainer.appendChild(transcriptHeader);
    const audioFileLink = document.createElement("a");
    audioFileLink.href = audioFile.path;
    audioFileLink.textContent = audioFile.path;
    this.contentContainer.appendChild(audioFileLink);
    this.contentContainer.appendChild(document.createElement("br"));
    const transcriptionParagraph = document.createElement("p");
    transcriptionParagraph.textContent = transcription;
    this.contentContainer.appendChild(transcriptionParagraph);
    this.removeButton();
  }
  removeButton() {
    if (this.buttonEl && this.buttonEl.parentNode) {
      this.buttonEl.parentNode.removeChild(this.buttonEl);
    }
  }
};

// src/processors/RecordBlockProcessor.ts
function registerRecordBlockProcessor(plugin, settings) {
  plugin.registerMarkdownCodeBlockProcessor("record", (source, el, ctx) => {
    const contentContainer = el.createDiv({ cls: "neurovox-record-content" });
    const floatingButton = new FloatingButton(plugin, settings);
    el.appendChild(floatingButton.buttonEl);
    el.neurovoxContentContainer = contentContainer;
    el.neurovoxFloatingButton = floatingButton;
    ctx.addChild(new class extends import_obsidian6.MarkdownRenderChild {
      constructor(containerEl) {
        super(containerEl);
      }
      onunload() {
        floatingButton.removeButton();
      }
    }(el));
  });
}

// src/main.ts
var NeuroVoxPlugin = class extends import_obsidian7.Plugin {
  /**
   * Runs when the plugin is loaded.
   * Initializes settings, UI components, and sets up event listeners.
   */
  async onload() {
    console.log("Loading NeuroVox plugin");
    await this.loadSettings();
    registerRecordBlockProcessor(this, this.settings);
    this.addSettingTab(new NeuroVoxSettingTab(this.app, this));
    document.documentElement.style.setProperty("--mic-button-color", this.settings.micButtonColor);
    new FloatingButton(this, this.settings);
    this.addCommand({
      id: "open-neurovox-view",
      name: "Open NeuroVox View",
      callback: () => {
        this.activateView();
      }
    });
  }
  /**
   * Runs when the plugin is unloaded.
   * Performs cleanup tasks.
   */
  onunload() {
    console.log("Unloading NeuroVox plugin");
  }
  /**
   * Loads the plugin settings.
   * Merges saved settings with default settings.
   */
  async loadSettings() {
    this.settings = Object.assign({}, DEFAULT_SETTINGS, await this.loadData());
  }
  /**
   * Saves the current plugin settings.
   */
  async saveSettings() {
    await this.saveData(this.settings);
  }
  /**
   * Activates the NeuroVox view.
   * Creates a new leaf for the view if it doesn't exist, or reveals an existing one.
   */
  async activateView() {
    const { workspace } = this.app;
    let leaf = workspace.getLeavesOfType("neurovox-view")[0];
    if (!leaf) {
      const newLeaf = workspace.getRightLeaf(false);
      if (newLeaf) {
        await newLeaf.setViewState({ type: "neurovox-view", active: true });
        leaf = newLeaf;
      } else {
        console.error("Failed to create a new leaf for NeuroVox view");
        return;
      }
    }
    workspace.revealLeaf(leaf);
  }
};
